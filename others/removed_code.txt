


device = "cuda" if torch.cuda.is_available() else "cpu"


q_transfer_ANN = QInterchangeNN().to(device)
q_runoff_ANN = QRunoffNN().to(device)


wn = DE.DynamicEmulator(inp_path, q_transfer_ANN, q_runoff_ANN)


#Draw graph
wn.draw_nx_layout()


wn.set_h(X_train[0][1])
columns = ['Time' , 'Node', 'Depth', 'x_coord' , 'y_coord']
df = pd.DataFrame(wn.get_depths_to_rows(0), columns = columns)

for time in range(1):
    wn.update_h(runoff = X_train[0][2][time]) 
    new_h_rows = pd.DataFrame(wn.get_depths_to_rows(time+1), columns = columns)
    df = pd.concat([df,new_h_rows])


depth_one_node = df[df['Node']=='j_90376']['Depth'] #.plot() j_90550
depth_one_node=depth_one_node.reset_index()
depth_one_node['Depth'].plot()


net = utils.animate_nodal_depth(df)
net.show()

net.write_html('10_06_2022_Depth_dynamic_viz.html')


losses_np=[]

optimizer1 = optim.SGD(wn.q_transfer_ANN.parameters(), lr=0.01, momentum=0.9)
optimizer2 = optim.SGD(wn.q_runoff_ANN.parameters(), lr=0.01, momentum=0.9)

time_steps = 1

for epoch in range(20):
    epoch_loss =[]
    for i in range(len(X_train)):
        
        wn.set_h(X_train[i][1])
        losses= []
        for time in range(time_steps):
            wn.update_h(runoff = X_train[i][2][time]) 

            pred = torch.cat(list(wn.get_h().values()))
            target = torch.reshape(torch.tensor(list(y_train[i][time].values())), (-1,1))

            loss = nn.MSELoss()(pred, target)
            losses.append(loss)

        total_loss = sum(losses)
        epoch_loss.append(total_loss.detach())
        
        if total_loss.requires_grad:
            # Backpropagate and update weights
            total_loss.backward()
            optimizer1.step()
            optimizer1.zero_grad()#set_to_none=True) 

            optimizer2.step()
            optimizer2.zero_grad()#set_to_none=True)
              
    losses_np.append(np.array(epoch_loss).mean())




pd.Series(np.array(losses_np)).plot()



# wn.set_h(X_train[0][1])
# columns = ['Time' , 'Node', 'Depth', 'x_coord' , 'y_coord']
# df = pd.DataFrame(wn.get_depths_to_rows(0), columns = columns)
# dummy_validation_rain = [0,0,0,2, 3, 4, 5, 0, 0,10,0 ,0, 0, 10, 10, 20, 30, 0,0 ,0,0,0,0,0,0,0,0,0]
# for time in range(len(dummy_validation_rain)):
#     wn.update_h(rain = (dummy_validation_rain[time])/40) 
#     new_h_rows = pd.DataFrame(wn.get_depths_to_rows(time+1), columns = columns)
#     df = pd.concat([df,new_h_rows])
%time
wn.set_h(X_train[0][1])
columns = ['Time' , 'Node', 'Depth', 'x_coord' , 'y_coord']
df = pd.DataFrame(wn.get_depths_to_rows(0), columns = columns)
# dummy_validation_rain = [0,0,0,2, 3, 4, 5, 0, 0,10,0 ,0, 0, 10, 10, 20, 30, 0,0 ,0,0,0,0,0,0,0,0,0]
# training_rainfall = dummy_validation_rain#list(rainfall_raw_data.value)

runoffs = [i[2][0] for i in x_values]

for time in range(len(runoffs)):
    wn.update_h(runoff = runoffs[time]) 
    new_h_rows = pd.DataFrame(wn.get_depths_to_rows(time+1), columns = columns)
    df = pd.concat([df,new_h_rows])



net.write_html('RFTrained_Depth_dynamic_viz.html')










fig = utils.plot_head_changes(node_name, single_node_x, single_node_y)
fig.show()



# For a single node
node_name = 'j_90492'
single_node_x = []
single_node_y = []
length_database = len(normalized_x) #same as y_train
step=1
for i in range(length_database):
    single_node_x.append(normalized_x[i][node_name])
    single_node_y.append(normalized_y[i][step-1][node_name])



#It considers all heads with the same scaling
norm = utils.Normalizer(X_train, y_train)

normalized_x = norm.normalized_h0_in_x
normalized_y = norm.normalized_ht_in_y







# set aside 20% of train and test data for evaluation
X_train, X_test, y_train, y_test = train_test_split(x_values, y_values,
    test_size=0.2, shuffle = True, random_state = 8)

# Use the same function above for the validation set
X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, 
    test_size=0.25, random_state= 8) # 0.25 x 0.8 = 0.2


x_values = [i[0] for i in couples_x0_ht_1steps]
y_values = [i[1] for i in couples_x0_ht_1steps]



# rolled_out_target_hydraulic_heads = []

# for dry_period in val_dry_period_indexes:
#     heads_no_rain = head_raw_data.iloc[list(dry_period), :]
#     heads = utils.get_rolled_out_target_hydraulic_heads(heads_no_rain)
    
#     rolled_out_target_hydraulic_heads.append(heads)


pd.DataFrame(history)['loss'].plot()
pd.DataFrame(history)['val_loss'].plot()


example = 205
training_windows[example].y - gnn_model(training_windows[example])





trial_window = training_windows[0]
normalized_trial_window =normalizer.normalize_window(trial_window)








    # def test_GNN_output_above_ground(self):
    #     output = self.GNN_model(self.trial_window)
    #     min_elev = self.trial_window['norm_elev']
    #     all_heads_are_above_ground = torch.all(torch.ge(output, min_elev))
        
    #     self.assertTrue(all_heads_are_above_ground.item())
        
    # def test_vector_says_if_dry_or_not(self):
    #     output = self.GNN_model(self.trial_window)
    #     # min_elev = self.trial_window['norm_elev']
    #     some_node_is_dry = torch.all(output)
        
    #     self.assertTrue(some_node_is_dry.item())

    # def test_vector_there_should_be_flow_to(self):
    #     output = self.GNN_model(self.trial_window)
    #     print(output)
    #     someflow = torch.any(output)
    #     self.assertTrue(someflow.item())
        
        
    # def test_vector_is_min_or_20(self):
    #     output = self.GNN_model(self.trial_window)
    #     print(output.shape)
    #     for i in output:
    #         val = i.item()
    #         self.assertTrue((val==0.0 or val == 1.0))


# def test_GNN_has_trainable_parameters(self):
#     self.assertGreater(utils.count_parameters(self.GNN_model), 0)





# def normalize_min_max(value, max,min):
#     return (value - min)/(max-min)



# class Normalizer:
#     def __init__(self, X_train, y_train):
#         self.initial_heads = [i[1] for i in X_train]
#         self.y_train = y_train
#         self.max_h, self.min_h = self.get_max_and_min_h()
#         self.normalized_h0_in_x = self.normalize_x_heads()
#         self.normalized_ht_in_y = self.normalize_y_heads()

#     def get_max_and_min_h(self):
#         samples_final = []
#         samples_final.extend(self.initial_heads)
        
#         for i in self.y_train:
#             samples_final+=i
        
#         max = np.array(list(samples_final[0].values())).max()
#         min = np.array(list(samples_final[0].values())).min()

#         for sample in samples_final:
#             val_max = np.array(list(sample.values())).max()
#             val_min = np.array(list(sample.values())).min()
#             if val_max>max:
#                 max = val_max
#             if val_min<min:
#                 min = val_min

#         return max, min

#     def normalize_x_heads(self):       
#         normalized_samples = []
#         for sample in self.initial_heads:
#             normalized_sample = dict(map(lambda x: (x[0], normalize_min_max(x[1], self.max_h, self.min_h )), sample.items()))
#             normalized_samples.append(normalized_sample)
#         return normalized_samples


#     def normalize_y_heads(self):
#         normalized_list = []
#         for heads_in_step in self.y_train:
#             normalized_samples = []
#             for sample in heads_in_step:
#                 normalized_sample = dict(map(lambda x: (x[0], normalize_min_max(x[1], self.max_h, self.min_h )), sample.items()))
#                 normalized_samples.append(normalized_sample)
#             normalized_list.append(normalized_samples)
#         return normalized_list











self.node_names = nx.get_node_attributes(self.G, 'nodes_names')






 # for i in range(len(heads)-steps_ahead):

        #     h0_samples = heads.iloc[i,:].to_dict()
            
        #     ro_samples = [runoff.iloc[i].to_dict()]
        #     ht_samples = []
            
        #     for j in range(steps_ahead):    
        #         ht = heads.iloc[i+j+1,:].to_dict()
        #         ht_samples.append(ht)
                                
        #         ro = runoff.iloc[i+j+1].to_dict()
        #         ro_samples.append(ro)

        #     x0_samples = [h0_samples, ro_samples]

        #     couple=(x0_samples, ht_samples)
        #     couples.append(couple)

        # return couples





#SWMMEmulator----------------------------------------------------------------
# import torch
# import networkx as nx
# from utils.SWMM_Simulation import SWMMSimulation
# import utils.head_change_utils as utils

# class SWMMEmulator:
#     def __init__(self, inp_path):
#         self.inp_lines = utils.get_lines_from_textfile(inp_path)
#         self.G = utils.inp_to_G(self.inp_lines)
        
#         self.original_min =     convert_dict_values_to_torch( nx.get_node_attributes(self.G, 'elevation') )
#         self.original_A_catch = convert_dict_values_to_torch( nx.get_node_attributes(self.G, 'area_subcatchment') )
#         self.pos = nx.get_node_attributes(self.G, 'pos')

#     def create_simulation(self, rainfall_raw_data, heads_raw_data, runoff_raw_data):
#         simulation = SWMMSimulation(rainfall_raw_data, heads_raw_data, runoff_raw_data)
#         return(simulation)
    


# def to_torch(object_to_convert):
#     return torch.tensor(float(object_to_convert), dtype=torch.float32)

# def convert_dict_values_to_torch(d):
#     dict_torch = {k:to_torch(v) for k,v in d.items()}
#     return dict_torch
#----------------------------------------------------------------







# def q_interchange_in(dh, L, d):
#     """
#     This function evaluates the magnitude that the difference in head has in the next head. How water flows.
#     """

#     num = (d**(5/2))*(dh**0.5) #**2.0)
#     den = L**0.5
#     q = w_in*(num/den)
    
#     return q


# def q_interchange_out(dh, L, d):
#     """
#     This function evaluates the magnitude that the difference in head has in the next head. How water flows.
#     """

#     num = (d**(5/2))*(dh**0.5) #**2.0)
#     den = L**0.5
#     q = w_out*(num/den)
    
#     return -q